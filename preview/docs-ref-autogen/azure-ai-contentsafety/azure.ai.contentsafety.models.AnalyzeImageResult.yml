### YamlMime:PythonClass
uid: azure.ai.contentsafety.models.AnalyzeImageResult
name: AnalyzeImageResult
fullName: azure.ai.contentsafety.models.AnalyzeImageResult
module: azure.ai.contentsafety.models
inheritances:
- azure.ai.contentsafety._model_base.Model
summary: The analysis response of the image.
constructor:
  syntax: 'AnalyzeImageResult(*args: Any, **kwargs: Any)'
variables:
- description: Analysis result for Hate category.
  name: hate_result
  types:
  - <xref:azure.ai.contentsafety.models.ImageAnalyzeSeverityResult>
- description: Analysis result for SelfHarm category.
  name: self_harm_result
  types:
  - <xref:azure.ai.contentsafety.models.ImageAnalyzeSeverityResult>
- description: Analysis result for Sexual category.
  name: sexual_result
  types:
  - <xref:azure.ai.contentsafety.models.ImageAnalyzeSeverityResult>
- description: Analysis result for Violence category.
  name: violence_result
  types:
  - <xref:azure.ai.contentsafety.models.ImageAnalyzeSeverityResult>
attributes:
- uid: azure.ai.contentsafety.models.AnalyzeImageResult.hate_result
  name: hate_result
  summary: Analysis result for Hate category.
  signature: 'hate_result: _models.ImageAnalyzeSeverityResult | None'
- uid: azure.ai.contentsafety.models.AnalyzeImageResult.self_harm_result
  name: self_harm_result
  summary: Analysis result for SelfHarm category.
  signature: 'self_harm_result: _models.ImageAnalyzeSeverityResult | None'
- uid: azure.ai.contentsafety.models.AnalyzeImageResult.sexual_result
  name: sexual_result
  summary: Analysis result for Sexual category.
  signature: 'sexual_result: _models.ImageAnalyzeSeverityResult | None'
- uid: azure.ai.contentsafety.models.AnalyzeImageResult.violence_result
  name: violence_result
  summary: Analysis result for Violence category.
  signature: 'violence_result: _models.ImageAnalyzeSeverityResult | None'
