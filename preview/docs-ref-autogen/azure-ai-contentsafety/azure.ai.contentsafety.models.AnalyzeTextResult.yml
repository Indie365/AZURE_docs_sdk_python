### YamlMime:PythonClass
uid: azure.ai.contentsafety.models.AnalyzeTextResult
name: AnalyzeTextResult
fullName: azure.ai.contentsafety.models.AnalyzeTextResult
module: azure.ai.contentsafety.models
inheritances:
- azure.ai.contentsafety._model_base.Model
summary: The analysis response of the text.
constructor:
  syntax: 'AnalyzeTextResult(*args: Any, **kwargs: Any)'
variables:
- description: The details of blocklist match.
  name: blocklists_match_results
  types:
  - <xref:list>[<xref:azure.ai.contentsafety.models.TextBlocklistMatchResult>]
- description: Analysis result for Hate category.
  name: hate_result
  types:
  - <xref:azure.ai.contentsafety.models.TextAnalyzeSeverityResult>
- description: Analysis result for SelfHarm category.
  name: self_harm_result
  types:
  - <xref:azure.ai.contentsafety.models.TextAnalyzeSeverityResult>
- description: Analysis result for Sexual category.
  name: sexual_result
  types:
  - <xref:azure.ai.contentsafety.models.TextAnalyzeSeverityResult>
- description: Analysis result for Violence category.
  name: violence_result
  types:
  - <xref:azure.ai.contentsafety.models.TextAnalyzeSeverityResult>
attributes:
- uid: azure.ai.contentsafety.models.AnalyzeTextResult.blocklists_match_results
  name: blocklists_match_results
  summary: The details of blocklist match.
  signature: 'blocklists_match_results: List[_models.TextBlocklistMatchResult] | None'
- uid: azure.ai.contentsafety.models.AnalyzeTextResult.hate_result
  name: hate_result
  summary: Analysis result for Hate category.
  signature: 'hate_result: _models.TextAnalyzeSeverityResult | None'
- uid: azure.ai.contentsafety.models.AnalyzeTextResult.self_harm_result
  name: self_harm_result
  summary: Analysis result for SelfHarm category.
  signature: 'self_harm_result: _models.TextAnalyzeSeverityResult | None'
- uid: azure.ai.contentsafety.models.AnalyzeTextResult.sexual_result
  name: sexual_result
  summary: Analysis result for Sexual category.
  signature: 'sexual_result: _models.TextAnalyzeSeverityResult | None'
- uid: azure.ai.contentsafety.models.AnalyzeTextResult.violence_result
  name: violence_result
  summary: Analysis result for Violence category.
  signature: 'violence_result: _models.TextAnalyzeSeverityResult | None'
